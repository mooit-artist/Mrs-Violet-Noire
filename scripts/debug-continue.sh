#!/bin/bash

echo "ğŸ” Continue Extension Debugging Script"
echo "======================================"
echo ""

echo "ğŸ“‹ 1. Checking Ollama Status:"
if pgrep ollama > /dev/null; then
    echo "âœ… Ollama is running"
    ollama ps | head -5
else
    echo "âŒ Ollama is not running"
    echo "Run: ollama serve"
fi
echo ""

echo "ğŸ“‹ 2. Testing Ollama API:"
curl -s -o /dev/null -w "%{http_code}" http://localhost:11434 && echo " âœ… Ollama API accessible" || echo " âŒ Ollama API not accessible"
echo ""

echo "ğŸ“‹ 3. Available Models:"
ollama list | head -10
echo ""

echo "ğŸ“‹ 4. Continue Configuration:"
if [ -f ".continue/config.json" ]; then
    echo "âœ… Project config exists"
    echo "Models configured:"
    jq -r '.models[].title' .continue/config.json 2>/dev/null || echo "âŒ Invalid JSON in config"
else
    echo "âŒ No project config found"
fi
echo ""

echo "ğŸ“‹ 5. Global Continue Configuration:"
if [ -f "$HOME/.continue/config.yaml" ]; then
    echo "âœ… Global config exists"
    echo "Models configured:"
    grep -A1 "name:" ~/.continue/config.yaml | grep "name:" | head -5
else
    echo "âŒ No global config found"
fi
echo ""

echo "ğŸ“‹ 6. Testing Model Response:"
echo "Testing llama3:8b..."
curl -s -X POST http://localhost:11434/api/generate \
  -d '{"model": "llama3:8b", "prompt": "Hello! Test response.", "stream": false}' \
  | jq -r '.response' 2>/dev/null | head -3 || echo "âŒ Model test failed"
echo ""

echo "ğŸ¯ Next Steps:"
echo "1. If all checks pass, restart VS Code"
echo "2. Open test-continue.md and try Continue (Cmd+I)"
echo "3. If issues persist, check VS Code Continue extension logs"
